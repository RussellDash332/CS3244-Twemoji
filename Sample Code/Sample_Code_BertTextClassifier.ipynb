{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertTextClassifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "823dKuFEv3eo"
      },
      "source": [
        "%pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcRaCfsRwQsZ"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from pathlib import Path \n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "import random \n",
        "\n",
        "# fastai\n",
        "from fastai import *\n",
        "from fastai.text import *\n",
        "from fastai.callbacks import *\n",
        "\n",
        "# transformers\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
        "from transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\n",
        "from transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-HQf7_nwZ4v"
      },
      "source": [
        "import fastai\n",
        "import transformers\n",
        "print('fastai version :', fastai.__version__)\n",
        "print('transformers version :', transformers.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-OK_w9k-j8S"
      },
      "source": [
        "DATASETS_URL = {\n",
        "    'wikitext-2':   {'train': \"https://s3.amazonaws.com/datasets.huggingface.co/wikitext-2/train.txt\",\n",
        "                     'valid': \"https://s3.amazonaws.com/datasets.huggingface.co/wikitext-2/valid.txt\"},\n",
        "    'wikitext-103': {'train': \"https://s3.amazonaws.com/datasets.huggingface.co/wikitext-103/wiki.train.tokens\",\n",
        "                     'valid': \"https://s3.amazonaws.com/datasets.huggingface.co/wikitext-103/wiki.valid.tokens\"},\n",
        "    'simplebooks-2-raw': {'train': \"https://s3.amazonaws.com/datasets.huggingface.co/simplebooks-2-raw/train.txt\",\n",
        "                          'valid': \"https://s3.amazonaws.com/datasets.huggingface.co/simplebooks-2-raw/valid.txt\"},\n",
        "    'simplebooks-92-raw': {'train': \"https://s3.amazonaws.com/datasets.huggingface.co/simplebooks-92-raw/train.txt\",\n",
        "                           'valid': \"https://s3.amazonaws.com/datasets.huggingface.co/simplebooks-92-raw/valid.txt\"},\n",
        "    'imdb': {'train': \"https://s3.amazonaws.com/datasets.huggingface.co/aclImdb/train.txt\",\n",
        "             'test': \"https://s3.amazonaws.com/datasets.huggingface.co/aclImdb/test.txt\"},\n",
        "    'trec': {'train': \"https://s3.amazonaws.com/datasets.huggingface.co/trec/train.txt\",\n",
        "             'test': \"https://s3.amazonaws.com/datasets.huggingface.co/trec/test.txt\"},\n",
        "    }\n",
        "\n",
        "DATASETS_LABELS_URL = {\n",
        "    'imdb': {'train': \"https://s3.amazonaws.com/datasets.huggingface.co/aclImdb/train.labels.txt\",\n",
        "             'test': \"https://s3.amazonaws.com/datasets.huggingface.co/aclImdb/test.labels.txt\"},\n",
        "    'trec': {'train': \"https://s3.amazonaws.com/datasets.huggingface.co/trec/train.labels.txt\",\n",
        "             'test': \"https://s3.amazonaws.com/datasets.huggingface.co/trec/test.labels.txt\"},\n",
        "    }\n",
        "\n",
        "DATASETS_LABELS_CONVERSION = {\n",
        "    'imdb':         {'pos': 0, 'neg': 1},\n",
        "    'trec':         {'NUM': 0, 'LOC': 1, 'HUM': 2, 'DESC': 3, 'ENTY': 4, 'ABBR': 5},\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaoPyiaHwgXI"
      },
      "source": [
        "\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/loretoparisi/bert_text_classifier/master/data/imdb_kaggle/train.tsv', sep=\"\\t\", quoting=csv.QUOTE_ALL, engine=\"python\", quotechar='\"', encoding=\"utf-8\")\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/loretoparisi/bert_text_classifier/master/data/imdb_kaggle/test.tsv', sep=\"\\t\", quoting=csv.QUOTE_ALL, engine=\"python\", quotechar='\"', encoding=\"utf-8\")\n",
        "\n",
        "print(train.shape, test.shape)\n",
        "train.head()\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XLPTq1Axypb"
      },
      "source": [
        "MODEL_CLASSES = {\n",
        "    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n",
        "    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n",
        "    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n",
        "    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n",
        "    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnjmioTvLKfQ"
      },
      "source": [
        "# Parameters\n",
        "seed = 42\n",
        "use_fp16 = False\n",
        "bs = 16\n",
        "\n",
        "model_type = 'roberta'\n",
        "pretrained_model_name = 'roberta-base' # 'roberta-base-openai-detector'\n",
        "\n",
        "# model_type = 'bert'\n",
        "# pretrained_model_name='bert-base-uncased'\n",
        "\n",
        "# model_type = 'distilbert'\n",
        "# pretrained_model_name = 'distilbert-base-uncased-distilled-squad'#'distilbert-base-uncased'#'distilbert-base-uncased'\n",
        "\n",
        "#model_type = 'xlm'\n",
        "#pretrained_model_name = 'xlm-clm-enfr-1024'\n",
        "\n",
        "#model_type = 'xlnet'\n",
        "#pretrained_model_name = 'xlnet-base-cased'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jVrdvwPLQRZ"
      },
      "source": [
        "model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvRqyuhaLTHS"
      },
      "source": [
        "model_class.pretrained_model_archive_map.keys()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c15N-jR6LW_7"
      },
      "source": [
        "def seed_all(seed_value):\n",
        "    random.seed(seed_value) # Python\n",
        "    np.random.seed(seed_value) # cpu vars\n",
        "    torch.manual_seed(seed_value) # cpu  vars\n",
        "    \n",
        "    if torch.cuda.is_available(): \n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
        "        torch.backends.cudnn.deterministic = True  #needed\n",
        "        torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiPK_RFLLbBC"
      },
      "source": [
        "seed_all(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu8f2UPgLfGD"
      },
      "source": [
        "class TransformersBaseTokenizer(BaseTokenizer):\n",
        "    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n",
        "    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n",
        "        self._pretrained_tokenizer = pretrained_tokenizer\n",
        "        self.max_seq_len = pretrained_tokenizer.max_len\n",
        "        self.model_type = model_type\n",
        "\n",
        "    def __call__(self, *args, **kwargs): \n",
        "        return self\n",
        "\n",
        "    def tokenizer(self, t:str) -> List[str]:\n",
        "        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n",
        "        CLS = self._pretrained_tokenizer.cls_token\n",
        "        SEP = self._pretrained_tokenizer.sep_token\n",
        "        if self.model_type in ['roberta']:\n",
        "            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n",
        "        else:\n",
        "            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n",
        "        return [CLS] + tokens + [SEP]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8FVnLsvLjvk"
      },
      "source": [
        "transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
        "transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\n",
        "fastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BemsGxvLzwe"
      },
      "source": [
        "tokenizer_class.pretrained_vocab_files_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnROdf3AMFvg"
      },
      "source": [
        "class TransformersVocab(Vocab):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
        "        super(TransformersVocab, self).__init__(itos = [])\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def numericalize(self, t:Collection[str]) -> List[int]:\n",
        "        \"Convert a list of tokens `t` to their ids.\"\n",
        "        return self.tokenizer.convert_tokens_to_ids(t)\n",
        "        #return self.tokenizer.encode(t)\n",
        "\n",
        "    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n",
        "        \"Convert a list of `nums` to their tokens.\"\n",
        "        nums = np.array(nums).tolist()\n",
        "        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onUvhVSjMKp_"
      },
      "source": [
        "transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\n",
        "numericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n",
        "\n",
        "tokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)\n",
        "\n",
        "transformer_processor = [tokenize_processor, numericalize_processor]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQnxo0eNMWJC"
      },
      "source": [
        "pad_first = bool(model_type in ['xlnet'])\n",
        "pad_idx = transformer_tokenizer.pad_token_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjx_eCYzMbEQ"
      },
      "source": [
        "databunch = (TextList.from_df(train, cols='Phrase', processor=transformer_processor)\n",
        "             .split_by_rand_pct(0.1,seed=seed)\n",
        "             .label_from_df(cols= 'Sentiment')\n",
        "             .add_test(test)\n",
        "             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOagk4oKHVTy"
      },
      "source": [
        "print('[CLS] token :', transformer_tokenizer.cls_token)\n",
        "print('[SEP] token :', transformer_tokenizer.sep_token)\n",
        "print('[PAD] token :', transformer_tokenizer.pad_token)\n",
        "databunch.show_batch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P798q34BHcA_"
      },
      "source": [
        "print('[CLS] id :', transformer_tokenizer.cls_token_id)\n",
        "print('[SEP] id :', transformer_tokenizer.sep_token_id)\n",
        "print('[PAD] id :', pad_idx)\n",
        "test_one_batch = databunch.one_batch()[0]\n",
        "print('Batch shape : ',test_one_batch.shape)\n",
        "print(test_one_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SMDRFClHfxH"
      },
      "source": [
        "# defining our model architecture \n",
        "class CustomTransformerModel(nn.Module):\n",
        "    def __init__(self, transformer_model: PreTrainedModel):\n",
        "        super(CustomTransformerModel,self).__init__()\n",
        "        self.transformer = transformer_model\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \n",
        "        #attention_mask = (input_ids!=1).type(input_ids.type()) # Test attention_mask for RoBERTa\n",
        "        \n",
        "        logits = self.transformer(input_ids,\n",
        "                                attention_mask = attention_mask)[0]   \n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfLfKcxAHiYs"
      },
      "source": [
        "config = config_class.from_pretrained(pretrained_model_name)\n",
        "config.num_labels = 5\n",
        "config.use_bfloat16 = use_fp16\n",
        "print(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf5n9Y6zHnOR"
      },
      "source": [
        "transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\n",
        "# transformer_model = model_class.from_pretrained(pretrained_model_name, num_labels = 5)\n",
        "\n",
        "custom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woqc7pDPH0i6"
      },
      "source": [
        "from fastai.callbacks import *\n",
        "from transformers import AdamW\n",
        "\n",
        "learner = Learner(databunch, \n",
        "                  custom_transformer_model, \n",
        "                  opt_func = lambda input: AdamW(input,correct_bias=False), \n",
        "                  metrics=[accuracy])\n",
        "\n",
        "# Show graph of learner stats and metrics after each epoch.\n",
        "learner.callbacks.append(ShowGraph(learner))\n",
        "\n",
        "# Put learn in FP16 precision mode. --> Seems to not working\n",
        "if use_fp16: learner = learner.to_fp16()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpN9Hz7kH30f"
      },
      "source": [
        "print(learner.model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvxG4YSjIAs7"
      },
      "source": [
        "# For DistilBERT\n",
        "# list_layers = [learner.model.transformer.distilbert.embeddings,\n",
        "#                learner.model.transformer.distilbert.transformer.layer[0],\n",
        "#                learner.model.transformer.distilbert.transformer.layer[1],\n",
        "#                learner.model.transformer.distilbert.transformer.layer[2],\n",
        "#                learner.model.transformer.distilbert.transformer.layer[3],\n",
        "#                learner.model.transformer.distilbert.transformer.layer[4],\n",
        "#                learner.model.transformer.distilbert.transformer.layer[5],\n",
        "#                learner.model.transformer.pre_classifier]\n",
        "\n",
        "# For roberta-base\n",
        "list_layers = [learner.model.transformer.roberta.embeddings,\n",
        "              learner.model.transformer.roberta.encoder.layer[0],\n",
        "              learner.model.transformer.roberta.encoder.layer[1],\n",
        "              learner.model.transformer.roberta.encoder.layer[2],\n",
        "              learner.model.transformer.roberta.encoder.layer[3],\n",
        "              learner.model.transformer.roberta.encoder.layer[4],\n",
        "              learner.model.transformer.roberta.encoder.layer[5],\n",
        "              learner.model.transformer.roberta.encoder.layer[6],\n",
        "              learner.model.transformer.roberta.encoder.layer[7],\n",
        "              learner.model.transformer.roberta.encoder.layer[8],\n",
        "              learner.model.transformer.roberta.encoder.layer[9],\n",
        "              learner.model.transformer.roberta.encoder.layer[10],\n",
        "              learner.model.transformer.roberta.encoder.layer[11],\n",
        "              learner.model.transformer.roberta.pooler]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONipJ8kzINy9"
      },
      "source": [
        "learner.split(list_layers)\n",
        "num_groups = len(learner.layer_groups)\n",
        "print('Learner split in',num_groups,'groups')\n",
        "print(learner.layer_groups)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77bEbzQVIUmD"
      },
      "source": [
        "learner.save('untrain')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7jZOCkDIX1v"
      },
      "source": [
        "seed_all(seed)\n",
        "learner.load('untrain');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OltDLZjIIad4"
      },
      "source": [
        "learner.freeze_to(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhUKJKtBIdih"
      },
      "source": [
        "learner.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10zfzvafIsLU"
      },
      "source": [
        "learner.lr_find()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A697rmz2MezZ"
      },
      "source": [
        "learner.recorder.plot(skip_end=7,suggestion=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV5IZhJcMjLR"
      },
      "source": [
        "learner.fit_one_cycle(1,max_lr=2e-03,moms=(0.8,0.7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHqAxqjmYV9F"
      },
      "source": [
        "learner.save('first_cycle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcKthjX-YbSO"
      },
      "source": [
        "seed_all(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "480OI-4TYdeG"
      },
      "source": [
        "learner.load('first_cycle');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMXmnXOnYiHo"
      },
      "source": [
        "learner.freeze_to(-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3yEC0ewYlFF"
      },
      "source": [
        "lr = 1e-5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhv6mMy8YoL-"
      },
      "source": [
        "learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KDWAhY4ae1F"
      },
      "source": [
        "learner.save('second_cycle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8Rpt6K3ahZY"
      },
      "source": [
        "seed_all(seed)\n",
        "learner.load('second_cycle');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYmfhNQ1alSP"
      },
      "source": [
        "learner.freeze_to(-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_Lca2f9apep"
      },
      "source": [
        "learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNFnkd2gcZls"
      },
      "source": [
        "learner.save('third_cycle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViGNwHOFcblt"
      },
      "source": [
        "seed_all(seed)\n",
        "learner.load('third_cycle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrqF3p2CceVz"
      },
      "source": [
        "learner.unfreeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCNKO43HcgrJ"
      },
      "source": [
        "learner.fit_one_cycle(2, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmc3viqUnElD"
      },
      "source": [
        "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    the get_preds method does not yield the elements in order by default\n",
        "    we borrow the code from the RNNLearner to resort the elements into their correct order\n",
        "    \"\"\"\n",
        "    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n",
        "    sampler = [i for i in databunch.dl(ds_type).sampler]\n",
        "    reverse_sampler = np.argsort(sampler)\n",
        "    return preds[reverse_sampler, :]\n",
        "\n",
        "test_preds = get_preds_as_nparray(DatasetType.Test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WktiIvznyXO"
      },
      "source": [
        "sample_submission = pd.read_csv('https://raw.githubusercontent.com/loretoparisi/bert_text_classifier/master/data/imdb_kaggle/sampleSubmission.csv', quoting=csv.QUOTE_ALL, engine=\"python\", quotechar='\"', encoding=\"utf-8\")\n",
        "print(sample_submission.shape)\n",
        "sample_submission.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytrKMCKmoq49"
      },
      "source": [
        "sample_submission['Sentiment'] = np.argmax(test_preds,axis=1)\n",
        "sample_submission.to_csv(\"predictions.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtKSbVzRou8b"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFSixD0Box6z"
      },
      "source": [
        "sample_submission.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE3QoukZo2ME"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "def create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n",
        "    html = '<a href={filename}>{title}</a>'\n",
        "    html = html.format(title=title,filename=filename)\n",
        "    return HTML(html)\n",
        "\n",
        "# create a link to download the dataframe which was saved with .to_csv method\n",
        "create_download_link(filename='predictions.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}