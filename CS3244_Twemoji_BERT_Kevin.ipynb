{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Issue\n",
    "\n",
    "-. My computer is not strong enough to train the model\n",
    "-. The 1st epoch is still 0% even after waiting for 5 hours of execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible solutions:\n",
    "\n",
    "-. Use additional GPUs\n",
    "-. Execute the code in cloud services like AWS / GCP / Azure\n",
    "-. Modify the model parameters (most recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules for the Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-pretrained-bert in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (0.6.2)\n",
      "Requirement already satisfied: pytorch-nlp in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from pytorch-pretrained-bert) (1.19.2)\n",
      "Requirement already satisfied: torch>=0.4.1 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from pytorch-pretrained-bert) (1.9.1)\n",
      "Requirement already satisfied: boto3 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from pytorch-pretrained-bert) (1.17.85)\n",
      "Requirement already satisfied: regex in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from pytorch-pretrained-bert) (2020.10.15)\n",
      "Requirement already satisfied: requests in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from pytorch-pretrained-bert) (2.24.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from pytorch-pretrained-bert) (4.50.2)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\kevin christian\\appdata\\roaming\\python\\python38\\site-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.85 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (1.20.85)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (0.4.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from requests->pytorch-pretrained-bert) (1.25.11)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from botocore<1.21.0,>=1.20.85->boto3->pytorch-pretrained-bert) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.85->boto3->pytorch-pretrained-bert) (1.15.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: clang~=5.0 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorflow) (5.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\kevin christian\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorflow) (1.41.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorflow) (0.14.1)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorflow) (3.18.1)\n",
      "Requirement already satisfied: tensorboard~=2.6 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: keras~=2.6 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (50.3.1.post20201107)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: torch in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (1.9.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: keras in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.19.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kevin christian\\anaconda3\\lib\\site-packages (4.50.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement io (from versions: none)\n",
      "ERROR: No matching distribution found for io\n"
     ]
    }
   ],
   "source": [
    "# install\n",
    "!pip install pytorch-pretrained-bert pytorch-nlp\n",
    "!pip install tensorflow\n",
    "!pip install torch\n",
    "!pip install keras\n",
    "!pip install sklearn\n",
    "!pip install tqdm\n",
    "!pip install io\n",
    "\n",
    "\n",
    "# BERT imports\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering + Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust your path variables according to your files location\n",
    "\n",
    "path = r\"C:\\Users\\Kevin Christian\\Desktop\\NUS\\NUS\\Modules\\3rd Semester\\CS3244\\Project\\Datasets\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(path + \"train_text_emoji_clean.csv\", sep = '\\t')\n",
    "test = pd.read_csv(path + \"test_text_emoji_clean.csv\", sep = '\\t')\n",
    "valid = pd.read_csv(path + \"valid_text_emoji_clean.csv\", sep = '\\t')\n",
    "\n",
    "train[\"tweets\"] = train[\"annotations,tweets,labels\"]\n",
    "test[\"tweets\"] = test[\"annotations,tweets,labels\"]\n",
    "valid[\"tweets\"] = valid[\"annotations,tweets,labels\"]\n",
    "\n",
    "for i in range(len(train)):\n",
    "    info = train[\"annotations,tweets,labels\"][i].rsplit(\",\", 1)\n",
    "    labels = info[1]\n",
    "    semi = info[0]\n",
    "    final = semi.split(\",\", 1)\n",
    "    message = final[1]\n",
    "    train[\"tweets\"][i] = message\n",
    "\n",
    "for i in range(len(test)):\n",
    "    info = test[\"annotations,tweets,labels\"][i].rsplit(\",\", 1)\n",
    "    labels = info[1]\n",
    "    semi = info[0]\n",
    "    final = semi.split(\",\", 1)\n",
    "    message = final[1]\n",
    "    test[\"tweets\"][i] = message\n",
    "    \n",
    "for i in range(len(valid)):\n",
    "    info = valid[\"annotations,tweets,labels\"][i].rsplit(\",\", 1)\n",
    "    labels = info[1]\n",
    "    semi = info[0]\n",
    "    final = semi.split(\",\", 1)\n",
    "    message = final[1]\n",
    "    valid[\"tweets\"][i] = message\n",
    "    \n",
    "\n",
    "    \n",
    "train[\"annotations\"] = train[\"annotations,tweets,labels\"].astype(str).str[0]\n",
    "test[\"annotations\"] = test[\"annotations,tweets,labels\"].astype(str).str[0]\n",
    "valid[\"annotations\"] = valid[\"annotations,tweets,labels\"].astype(str).str[0]\n",
    "\n",
    "# Assign the messages as the main feature for prediction\n",
    "\n",
    "X_train = train[\"tweets\"]\n",
    "X_test = test[\"tweets\"]\n",
    "X_valid = valid[\"tweets\"]\n",
    "\n",
    "# Extract the first emoji only if there are multiple emojis\n",
    "\n",
    "#train['result'] = train['annotations'].astype(str).str[0]\n",
    "#test['result'] = test['annotations'].astype(str).str[0]\n",
    "#valid['result'] = valid['annotations'].astype(str).str[0]\n",
    "\n",
    "# Encode the classifications into integers\n",
    "\n",
    "train['classification'] = pd.factorize(train[\"annotations\"])[0]\n",
    "test['classification'] = pd.factorize(test[\"annotations\"])[0]\n",
    "valid['classification'] = pd.factorize(valid[\"annotations\"])[0]\n",
    "\n",
    "y_train = train['classification']\n",
    "y_test = test['classification']\n",
    "y_valid = valid['classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ðŸ˜‚', 'ðŸ˜­', 'ðŸ˜', 'ðŸ™„', 'â¤', 'ðŸ˜Š', 'ðŸ˜©', 'ðŸ¤”', 'ðŸ˜˜', 'ðŸ½', 'ðŸ’¯', 'ðŸ™ƒ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emojis = train[\"annotations\"].unique()\n",
    "\n",
    "# This emojis can be used to convert the predicted number into its respective emoji\n",
    "\n",
    "emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                               school is so dead o my god\n",
      "1        lol I've been told mine is worse than yours bu...\n",
      "2        I'm excited to hear them..... That shit is goi...\n",
      "3        Damn alycia knows everything even indirect tweets\n",
      "4                   x_juicebox: That sound like everything\n",
      "                               ...                        \n",
      "59900    FinesseYoAss: Or That Picture Was Just Cute To...\n",
      "59901                   I'm otw wit some gas still hate me\n",
      "59902                                  Yeahhh you kinda do\n",
      "59903                       nothing wit youuuuu raymundooo\n",
      "59904    dont be that person who parks right next to me...\n",
      "Name: tweets, Length: 59905, dtype: object\n",
      "==================================================\n",
      "0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "         ..\n",
      "59900    11\n",
      "59901    11\n",
      "59902    11\n",
      "59903    11\n",
      "59904    11\n",
      "Name: classification, Length: 59905, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "\n",
    "print(\"==================================================\")\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          The conversation is so deep i saw adele rolling\n",
      "1        \"Saras_OREO: Lord, we're coo asf lol Just wait...\n",
      "2                                                  I can't\n",
      "3        These little kids talking about going to Canad...\n",
      "4                                       only would do that\n",
      "                               ...                        \n",
      "15179                           The ball hates Croatia smh\n",
      "15180                        Sexy bitch I hope she bout it\n",
      "15181    I didn't realize that my tweets were private a...\n",
      "15182                         lowkey annoyed for no reason\n",
      "15183                          appreciate it while it last\n",
      "Name: tweets, Length: 15184, dtype: object\n",
      "==================================================\n",
      "0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "         ..\n",
      "15179    11\n",
      "15180    11\n",
      "15181    11\n",
      "15182    11\n",
      "15183    11\n",
      "Name: classification, Length: 15184, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_test)\n",
    "\n",
    "print(\"==================================================\")\n",
    "\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                             hahahah you guys crack me up\n",
      "1                                                 I'm dead\n",
      "2                    ate girl! The tweet was before TMC pa\n",
      "3        Also who would've thought that Will Griggs on ...\n",
      "4                           I'm not fooling w. you tonight\n",
      "                               ...                        \n",
      "15328    Hopefully I don't fall asleep in the toilets i...\n",
      "15329    Excited for the lake. Not excited to wake up a...\n",
      "15330    mom just told me shes leaving early to go to a...\n",
      "15331          all these double shifts are killing my vibe\n",
      "15332    KamryDeLeon: I love when my legs are shaved bu...\n",
      "Name: tweets, Length: 15333, dtype: object\n",
      "==================================================\n",
      "0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "         ..\n",
      "15328    11\n",
      "15329    11\n",
      "15330    11\n",
      "15331    11\n",
      "15332    11\n",
      "Name: classification, Length: 15333, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_valid)\n",
    "\n",
    "print(\"==================================================\")\n",
    "\n",
    "print(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] lol I've been told mine is worse than yours but I don't believe that [SEP]\n",
      "[CLS] \"Saras_OREO: Lord, we're coo asf lol Just waiting for break so we can chill\" [SEP]\n",
      "[CLS] I'm dead [SEP]\n"
     ]
    }
   ],
   "source": [
    "# In BERT, always remember to include [CLS] and [SEP] for every text.\n",
    "\n",
    "sentences_train = [\"[CLS] \" + query + \" [SEP]\" for query in X_train]\n",
    "sentences_test = [\"[CLS] \" + query + \" [SEP]\" for query in X_test]\n",
    "sentences_valid = [\"[CLS] \" + query + \" [SEP]\" for query in X_valid]\n",
    "print(sentences_train[1])\n",
    "print(sentences_test[1])\n",
    "print(sentences_valid[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence of the training:\n",
      "['[CLS]', 'school', 'is', 'so', 'dead', 'o', 'my', 'god', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#Tokenize all the sentences in the train, test, valid\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenized_texts_train = [tokenizer.tokenize(sent) for sent in sentences_train]\n",
    "tokenized_texts_test = [tokenizer.tokenize(sent) for sent in sentences_test]\n",
    "tokenized_texts_valid = [tokenizer.tokenize(sent) for sent in sentences_valid]\n",
    "print (\"Tokenize the first sentence of the training:\")\n",
    "print (tokenized_texts_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. \n",
    "MAX_LEN = 128\n",
    "# Pad our input tokens\n",
    "input_ids_train = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_train],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids_test = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_test],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids_valid = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_valid],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids_train = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts_train]\n",
    "input_ids_train = pad_sequences(input_ids_train, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids_test = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts_test]\n",
    "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids_valid = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts_valid]\n",
    "input_ids_valid = pad_sequences(input_ids_valid, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks_train = []\n",
    "attention_masks_test = []\n",
    "attention_masks_valid = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids_train:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks_train.append(seq_mask)\n",
    "\n",
    "for seq in input_ids_test:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks_test.append(seq_mask)\n",
    "\n",
    "for seq in input_ids_valid:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks_valid.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = input_ids_train\n",
    "test_inputs = input_ids_test\n",
    "validation_inputs = input_ids_valid\n",
    "\n",
    "train_labels = y_train\n",
    "test_labels = y_test\n",
    "validation_labels = y_valid\n",
    "\n",
    "train_masks = attention_masks_train\n",
    "test_masks = attention_masks_test\n",
    "validation_masks = attention_masks_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "# Select a batch size for training. \n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader \n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=12, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=12)\n",
    "\n",
    "model.parameters\n",
    "\n",
    "#model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT fine-tuning parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "  \n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "# Number of training epochs \n",
    "epochs = 4\n",
    "\n",
    "# BERT training loop\n",
    "for _ in trange(epochs, desc=\"Epoch\"):  \n",
    "  \n",
    "  ## TRAINING\n",
    "  \n",
    "  # Set our model to training mode\n",
    "  model.train()  \n",
    "  # Tracking variables\n",
    "  tr_loss = 0\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    #batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    train_loss_set.append(loss.item())    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "       \n",
    "  ## VALIDATION\n",
    "\n",
    "  # Put model in evaluation mode\n",
    "  model.eval()\n",
    "  # Tracking variables \n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps, nb_eval_examples = 0, 0\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    #batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "\n",
    "# plot training performance\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_loss_set)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  \n",
    "prediction_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "## Prediction on test set\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "  with torch.no_grad():\n",
    "    # Forward pass, calculate logit predictions\n",
    "    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "  \n",
    "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "matthews_set = []\n",
    "for i in range(len(true_labels)):\n",
    "  matthews = matthews_corrcoef(true_labels[i],\n",
    "                 np.argmax(predictions[i], axis=1).flatten())\n",
    "  matthews_set.append(matthews)\n",
    "  \n",
    "# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "print('Classification accuracy using BERT Fine Tuning: {0:0.2%}'.format(matthews_corrcoef(flat_true_labels, flat_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save The Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./best_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
